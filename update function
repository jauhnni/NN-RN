# Define the update function to update the network parameters over 1 iteration
def backprop_update(x, t, wh, wo, learning_rate):
    # Compute the output of the network
    # This can be done with y = nn(x, wh, wo), but we need the intermediate 
    #  h and zh for the weight updates.
    zh = np.dot(x ,wh)
    #h = rbf(zh)  # hidden_activations(x, wh)
    h = logistic(zh)
    #h=act_log(x ,wh)
    y = np.around(act_log(h, wo))
    # Compute the gradient at the output
    grad_output = gradient_output(y, t)
    # Get the delta for wo
    d_wo = learning_rate * gradient_weight_out(h, grad_output)
    # Compute the gradient at the hidden layer
    grad_hidden = gradient_hidden(wo, grad_output)
    # Get the delta for wh
    d_wh = learning_rate * gradient_weight_hidden(x, h, grad_hidden) 

    return (wh-d_wh, wo-d_wo)
